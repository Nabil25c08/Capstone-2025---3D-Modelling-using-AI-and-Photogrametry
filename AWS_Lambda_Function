import json
import boto3
import os

# Initialize the Batch client
batch = boto3.client('batch')

def lambda_handler(event, context):
    print("Received S3 Event...")
    
    # 1. Extract the Bucket Name and File Name (Key) from the event
    # This event is automatically generated by S3 when a file is uploaded
    s3_info = event['Records'][0]['s3']
    input_bucket = s3_info['bucket']['name']
    input_key = s3_info['object']['key']
    
    print(f"Processing File: {input_key} from Bucket: {input_bucket}")
    
    # 2. Get Configuration from Environment Variables (We will set these next)
    job_queue = os.environ.get('JOB_QUEUE')      # e.g., Scanner-Queue
    job_definition = os.environ.get('JOB_DEF')   # e.g., Scanner-Job-Def
    output_bucket = os.environ.get('OUTPUT_BUCKET')
    
    # 3. Submit the Job to AWS Batch
    try:
        response = batch.submit_job(
            jobName='automatic-scan-job', # Name can be anything
            jobQueue=job_queue,
            jobDefinition=job_definition,
            containerOverrides={
                'environment': [
                    {'name': 'INPUT_BUCKET', 'value': input_bucket},
                    {'name': 'INPUT_KEY', 'value': input_key},
                    {'name': 'OUTPUT_BUCKET', 'value': output_bucket}
                ]
            }
        )
        print(f"Job Submitted! Job ID: {response['jobId']}")
        return {
            'statusCode': 200,
            'body': json.dumps(f"Job submitted for {input_key}")
        }
    except Exception as e:
        print(f"Error submitting job: {e}")
        raise e
